{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv('NER dataset.csv', encoding='unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare train fastmodel\n",
    "sentences = [\"\"]\n",
    "last_id = 1\n",
    "for id, word in dataset[['Sentence #', 'Word']].iloc:\n",
    "    if not pd.isnull(id):\n",
    "        id = int(id[9:])\n",
    "        if last_id != id:\n",
    "            sentences[last_id - 1] += '\\n'\n",
    "            sentences.append(\"\")\n",
    "            last_id = id\n",
    "    sentences[last_id - 1] += ' ' + str(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = open('dataset.txt', 'w')\n",
    "out.writelines(sentences)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.fasttext.FastText object at 0x7f6539bb6210>\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint as print\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "# Set file names for train and test data\n",
    "corpus_file = 'dataset.txt'\n",
    "\n",
    "model = FastText(vector_size=100)\n",
    "\n",
    "# build the vocabulary\n",
    "model.build_vocab(corpus_file=corpus_file)\n",
    "\n",
    "# train the model\n",
    "model.train(\n",
    "    corpus_file=corpus_file, epochs=model.epochs,\n",
    "    total_examples=model.corpus_count, total_words=model.corpus_total_words,\n",
    ")\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_training = [[]]\n",
    "last_id = 1\n",
    "for id, word, tag in dataset[['Sentence #', 'Word', 'Tag']].iloc:\n",
    "    if not pd.isnull(id):\n",
    "        id = int(id[9:])\n",
    "        if last_id != id:\n",
    "            data_for_training.append([])\n",
    "            last_id = id\n",
    "    data_for_training[last_id - 1].append([str(word), tag])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Maximal word number in sentence is 104.'\n"
     ]
    }
   ],
   "source": [
    "max_text_width = max(map(lambda it: len(it), data_for_training))\n",
    "print(f'Maximal word number in sentence is {max_text_width}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "fasttext_model_name = \"model.model\"\n",
    "\n",
    "fasttext_model = FastText.load('model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-art',\n",
       " 'B-eve',\n",
       " 'B-geo',\n",
       " 'B-gpe',\n",
       " 'B-nat',\n",
       " 'B-org',\n",
       " 'B-per',\n",
       " 'B-tim',\n",
       " 'I-art',\n",
       " 'I-eve',\n",
       " 'I-geo',\n",
       " 'I-gpe',\n",
       " 'I-nat',\n",
       " 'I-org',\n",
       " 'I-per',\n",
       " 'I-tim']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_of_labels = set()\n",
    "for cur_sample in data_for_training:\n",
    "    set_of_labels |= set(map(lambda it: it[1], cur_sample))\n",
    "set_of_labels -= {'O'}\n",
    "set_of_labels = ['O'] +  sorted(list(set_of_labels))\n",
    "set_of_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dictionary of unique words is created...'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f3dabe5799471eb5b0497b749bdeaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38367 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'There are 31886 unique words in the training data!'\n",
      "'Vectors of unique words are calculated...'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc6012d95c64fb7ac220041abcf0a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31887 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dictionary of unique words is created...'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f22aa9bc6374702b3b1fc3e933c1636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9592 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'There are 17207 unique words in the training data!'\n",
      "'Vectors of unique words are calculated...'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e18bb4ed9d4f6eb9ab48107c4144b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class TrainsetGenerator(Dataset):\n",
    "    def __init__(self, training_set,\n",
    "                 classes_list,\n",
    "                 used_fasttext_model: FastText, batch_size: int):\n",
    "        self.training_set = sorted(training_set, key=lambda it: len(it))\n",
    "        self.classes_list = classes_list\n",
    "        self.batch_size = batch_size\n",
    "        self.vocabulary_ = {'<BOS>': 0, '<EOS>': 1}\n",
    "        n_words = 2\n",
    "        classes_set = set(self.classes_list)\n",
    "        print(f'Dictionary of unique words is created...')\n",
    "        for cur_sent in tqdm(self.training_set):\n",
    "            for cur_word, cur_label in cur_sent:\n",
    "                if cur_word not in self.vocabulary_:\n",
    "                    self.vocabulary_[cur_word] = n_words\n",
    "                    n_words += 1\n",
    "                if cur_label not in classes_set:\n",
    "                    err_msg_ = f'Label \"{cur_label}\" is unknown!'\n",
    "                    raise ValueError(err_msg_)\n",
    "        del classes_set\n",
    "        print(f'There are {n_words - 2} unique words in the training data!')\n",
    "        all_words = sorted(list(self.vocabulary_.keys()))\n",
    "        word_vector = used_fasttext_model.wv[all_words[0]].astype(np.float64)\n",
    "        word_vector /= np.linalg.norm(word_vector)\n",
    "        word_vector = word_vector.astype(np.float32)\n",
    "        word_idx = self.vocabulary_[all_words[0]]\n",
    "        self.vector_size_ = word_vector.shape[0] + 2\n",
    "        self.matrix_ = np.zeros((n_words, self.vector_size_), dtype=np.float32)\n",
    "        self.matrix_[0, self.vector_size_ - 2] = 1.0\n",
    "        self.matrix_[1, self.vector_size_ - 1] = 1.0\n",
    "        self.matrix_[word_idx, 0:(self.vector_size_ - 2)] = word_vector\n",
    "        del word_vector\n",
    "        print(f'Vectors of unique words are calculated...')\n",
    "        for cur_word in tqdm(all_words[1:]):\n",
    "            word_vector = used_fasttext_model.wv[cur_word].astype(np.float64)\n",
    "            word_vector /= np.linalg.norm(word_vector)\n",
    "            word_vector = word_vector.astype(np.float32)\n",
    "            word_idx = self.vocabulary_[cur_word]\n",
    "            self.matrix_[word_idx, 0:(self.vector_size_ - 2)] = word_vector\n",
    "            del word_vector\n",
    "        del all_words\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.training_set) // self.batch_size)\n",
    "\n",
    "    def __getitem__(self, batch_idx):\n",
    "        batch_start = batch_idx * self.batch_size\n",
    "        batch_end = min(len(self.training_set), batch_start + self.batch_size)\n",
    "        max_text_len = max(map(\n",
    "            lambda it: len(it),\n",
    "            self.training_set[batch_start:batch_end]\n",
    "        ))\n",
    "        max_text_len += 2\n",
    "        batch_x = np.zeros(\n",
    "            (batch_end - batch_start, max_text_len, self.vector_size_),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        batch_y = np.zeros(\n",
    "            (batch_end - batch_start, max_text_len),\n",
    "            dtype=np.int64\n",
    "        )\n",
    "        for sent_idx, cur_sent in enumerate(self.training_set[batch_start:batch_end]):\n",
    "            time_idx = 0\n",
    "            word_idx = self.vocabulary_['<BOS>']\n",
    "            batch_x[sent_idx, time_idx] = self.matrix_[word_idx]\n",
    "            class_idx = self.classes_list.index('O')\n",
    "            batch_y[sent_idx, time_idx] = class_idx\n",
    "            for time_idx, (cur_word, cur_label) in enumerate(cur_sent):\n",
    "                word_idx = self.vocabulary_[cur_word]\n",
    "                batch_x[sent_idx, time_idx + 1] = self.matrix_[word_idx]\n",
    "                class_idx = self.classes_list.index(cur_label)\n",
    "                batch_y[sent_idx, time_idx + 1] = class_idx\n",
    "            time_idx = len(cur_sent) + 1\n",
    "            word_idx = self.vocabulary_['<EOS>']\n",
    "            batch_x[sent_idx, time_idx] = self.matrix_[word_idx]\n",
    "            class_idx = self.classes_list.index('O')\n",
    "            batch_y[sent_idx, time_idx] = class_idx\n",
    "        return torch.tensor(batch_x), torch.tensor(batch_y)\n",
    "\n",
    "random.shuffle(data_for_training)\n",
    "n_valid = int(round(0.2 * len(data_for_training)))\n",
    "n_train = len(data_for_training) - n_valid\n",
    "train_dataloader = TrainsetGenerator(data_for_training[:n_train], set_of_labels, fasttext_model, batch_size=512)\n",
    "valid_dataloader = TrainsetGenerator(data_for_training[n_train:], set_of_labels, fasttext_model, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (rnn): LSTM(102, 128, batch_first=True, bidirectional=True)\n",
       "  (cls): Linear(in_features=256, out_features=17, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(102, 128, batch_first=True, bidirectional=True)\n",
    "        self.cls = nn.Linear(256, len(set_of_labels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "        return self.cls(rnn_out)\n",
    "\n",
    "model = Model()\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 200\n",
    "lr=0.005\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "\n",
    "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Epoch: 1/200.............'\n",
      "'Loss: 0.7390093396644335 Valid: 0.42496255211818196'\n",
      "'Epoch: 2/200.............'\n",
      "'Loss: 0.2787059611968092 Valid: 0.21513677754117572'\n",
      "'Epoch: 3/200.............'\n",
      "'Loss: 0.19281328429241437 Valid: 0.1861499468166263'\n",
      "'Epoch: 4/200.............'\n",
      "'Loss: 0.17183759828677048 Valid: 0.1691195695087289'\n",
      "'Epoch: 5/200.............'\n",
      "'Loss: 0.1592262248332436 Valid: 0.15893687764944536'\n",
      "'Epoch: 6/200.............'\n",
      "'Loss: 0.15001546067965998 Valid: 0.15146216019391018'\n",
      "'Epoch: 7/200.............'\n",
      "'Loss: 0.14256515303576314 Valid: 0.14543563564726228'\n",
      "'Epoch: 8/200.............'\n",
      "'Loss: 0.13629456116138278 Valid: 0.14028495802965407'\n",
      "'Epoch: 9/200.............'\n",
      "'Loss: 0.13088562702004974 Valid: 0.1359817224223546'\n",
      "'Epoch: 10/200.............'\n",
      "'Loss: 0.12611031471877485 Valid: 0.13253614204766256'\n",
      "'Epoch: 11/200.............'\n",
      "'Loss: 0.12176967935787665 Valid: 0.12952024194230022'\n",
      "'Epoch: 12/200.............'\n",
      "'Loss: 0.11778036857376227 Valid: 0.12690466368113815'\n",
      "'Epoch: 13/200.............'\n",
      "'Loss: 0.11416101787944098 Valid: 0.12467028945373473'\n",
      "'Epoch: 14/200.............'\n",
      "'Loss: 0.11086996564188518 Valid: 0.12286946131771614'\n",
      "'Epoch: 15/200.............'\n",
      "'Loss: 0.10782148699099953 Valid: 0.12140537604988119'\n",
      "'Epoch: 16/200.............'\n",
      "'Loss: 0.10495092991638828 Valid: 0.12024178797183231'\n",
      "'Epoch: 17/200.............'\n",
      "'Loss: 0.10222239837654538 Valid: 0.11937136299447941'\n",
      "'Epoch: 18/200.............'\n",
      "'Loss: 0.09961409690613682 Valid: 0.11869972469580542'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mikhailp/Documents/NLP/lab2/solution.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mikhailp/Documents/NLP/lab2/solution.ipynb#X16sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(tag_scores\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39mlen\u001b[39m(set_of_labels)), tags\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mikhailp/Documents/NLP/lab2/solution.ipynb#X16sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     loss_number \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/mikhailp/Documents/NLP/lab2/solution.ipynb#X16sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mikhailp/Documents/NLP/lab2/solution.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mikhailp/Documents/NLP/lab2/solution.ipynb#X16sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m val_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/nn/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/nn/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_loss = 10\n",
    "state_dict = {}\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss_number = 0\n",
    "    for i in range(len(train_dataloader)):\n",
    "        sentence, tags = train_dataloader[i]\n",
    "        sentence, tags = sentence.to('cuda'), tags.to('cuda')\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence)\n",
    "        \n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = criterion(tag_scores.view(-1,len(set_of_labels)), tags.view(-1))\n",
    "        loss_number += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(valid_dataloader)):\n",
    "            x, y = valid_dataloader[i]\n",
    "            x, y = x.to('cuda'), y.to('cuda')\n",
    "            output = model(x)\n",
    "            loss = criterion(output.view(-1,len(set_of_labels)), y.view(-1))\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= len(valid_dataloader)\n",
    "    if best_loss > val_loss:\n",
    "        state_dict = model.state_dict()\n",
    "        best_loss = val_loss\n",
    "        \n",
    "    if epoch%1 == 0:\n",
    "        print('Epoch: {}/{}.............'.format(epoch, n_epochs))\n",
    "        print(\"Loss: {} Valid: {}\".format(loss_number/len(train_dataloader), val_loss))\n",
    "    loss_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for i in range(len(valid_dataloader)):\n",
    "        sentence, tags = valid_dataloader[i]\n",
    "        sentence, tags = sentence.to('cuda'), tags.to('cuda')\n",
    "        predicted = np.argmax(model(sentence).cpu(), axis=2)\n",
    "        for p, t in zip(predicted, tags):\n",
    "            for p1, t1 in zip(p, t):\n",
    "                true_labels.append(set_of_labels[t1])\n",
    "                predicted_labels.append(set_of_labels[p1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikhailp/anaconda3/envs/nn/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mikhailp/anaconda3/envs/nn/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('              precision    recall  f1-score   support\\n'\n",
      " '\\n'\n",
      " '       B-art       0.00      0.00      0.00        84\\n'\n",
      " '       B-eve       0.76      0.22      0.34        60\\n'\n",
      " '       B-geo       0.82      0.88      0.85      7416\\n'\n",
      " '       B-gpe       0.92      0.88      0.90      3220\\n'\n",
      " '       B-nat       0.80      0.08      0.15        50\\n'\n",
      " '       B-org       0.71      0.63      0.66      4025\\n'\n",
      " '       B-per       0.81      0.73      0.77      3480\\n'\n",
      " '       B-tim       0.93      0.86      0.89      4036\\n'\n",
      " '       I-art       0.00      0.00      0.00        58\\n'\n",
      " '       I-eve       0.50      0.08      0.14        49\\n'\n",
      " '       I-geo       0.79      0.68      0.73      1497\\n'\n",
      " '       I-gpe       0.89      0.39      0.54        41\\n'\n",
      " '       I-nat       0.00      0.00      0.00        11\\n'\n",
      " '       I-org       0.73      0.69      0.71      3298\\n'\n",
      " '       I-per       0.79      0.86      0.82      3483\\n'\n",
      " '       I-tim       0.81      0.72      0.76      1214\\n'\n",
      " '           O       0.99      0.99      0.99    196730\\n'\n",
      " '\\n'\n",
      " '    accuracy                           0.96    228752\\n'\n",
      " '   macro avg       0.66      0.51      0.54    228752\\n'\n",
      " 'weighted avg       0.96      0.96      0.96    228752\\n')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikhailp/anaconda3/envs/nn/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(true_labels, predicted_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
