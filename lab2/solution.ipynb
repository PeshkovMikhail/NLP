{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://gmb.let.rug.nl/releases/gmb-1.0.0.zip\n",
    "!unzip gmb-1.0.0.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(f):\n",
    "    res = \"\"\n",
    "\n",
    "    for line in f:\n",
    "        for i in '\\n.,:;!\"-=+/':\n",
    "            line = line.replace(i, ' ')\n",
    "        line = line.replace(\"'\", \" '\")\n",
    "        res += ' ' + line\n",
    "    return ' '.join(res.split()) + '\\n'\n",
    "\n",
    "dataset = []\n",
    "limit = 10000\n",
    "\n",
    "for i in os.walk(\"gmb-1.0.0/\"):\n",
    "    if not 'en.raw' in i[2]:\n",
    "        continue\n",
    "    if len(dataset) == limit:\n",
    "        break\n",
    "    f = open(f'{i[0]}/en.raw', 'r')\n",
    "    dataset.append(prepare(f))\n",
    "    f.close()\n",
    "out = open('dataset.txt', 'w')\n",
    "out.writelines(dataset)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.fasttext.FastText object at 0x7f0ea55b3450>\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint as print\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "# Set file names for train and test data\n",
    "corpus_file = 'dataset.txt'\n",
    "\n",
    "model = FastText(vector_size=100)\n",
    "\n",
    "# build the vocabulary\n",
    "model.build_vocab(corpus_file=corpus_file)\n",
    "\n",
    "# train the model\n",
    "model.train(\n",
    "    corpus_file=corpus_file, epochs=model.epochs,\n",
    "    total_examples=model.corpus_count, total_words=model.corpus_total_words,\n",
    ")\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.fasttext.FastText object at 0x7f7901c90210>\n"
     ]
    }
   ],
   "source": [
    "# from pprint import pprint as print\n",
    "# from gensim.models.fasttext import FastText\n",
    "# from gensim.test.utils import datapath\n",
    "\n",
    "# # Set file names for train and test data\n",
    "# corpus_file = datapath('lee_background.cor')\n",
    "\n",
    "# model = FastText(vector_size=100)\n",
    "\n",
    "# # build the vocabulary\n",
    "# model.build_vocab(corpus_file=corpus_file)\n",
    "\n",
    "# # train the model\n",
    "# model.train(\n",
    "#     corpus_file=corpus_file, epochs=model.epochs,\n",
    "#     total_examples=model.corpus_count, total_words=model.corpus_total_words,\n",
    "# )\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "\n",
    "def tsne_plot(labels, tokens, classes, clusters):\n",
    "    tsne_model = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=33)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "\n",
    "    colors = cm.rainbow(np.linspace(0, 1, clusters))\n",
    "    \n",
    "    plt.figure(figsize=(16, 9))\n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i], y[i], c=colors[classes[i]].reshape(1,-1), alpha=0.75)\n",
    "        plt.annotate(labels[i], alpha=0.75, xy=(x[i], y[i]), xytext=(5, 2), \n",
    "                     textcoords='offset points', ha='right', va='bottom', size=10)\n",
    "        \n",
    "    plt.grid(True)\n",
    "    plt.savefig('embedding.png', dpi=300)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import os\n",
    "\n",
    "def prepare(root):\n",
    "    ner_tags = list(filter(lambda tag: tag.attrib['type'] == 'ne', tree.getroot().iter('tag')))\n",
    "    words = list(root.iter('word'))\n",
    "    sentenses = []\n",
    "    sentence = []\n",
    "    i = '1'\n",
    "    for word in words:\n",
    "        a = word.attrib\n",
    "        if a[\"{http://www.w3.org/XML/1998/namespace}id\"][1] != i:\n",
    "            i = a[\"{http://www.w3.org/XML/1998/namespace}id\"][1]\n",
    "            sentenses.append(sentence)\n",
    "            sentence = []\n",
    "        sentence.append([word.text, 'O'])\n",
    "    sentenses.append(sentence)\n",
    "    \n",
    "    for tag in ner_tags:\n",
    "        sentense_id = int(tag.attrib['index'][1])-1\n",
    "        pos = int(tag.attrib['index'][2:])-1\n",
    "        sentenses[sentense_id][pos][1] = tag.text\n",
    "\n",
    "    return sentenses\n",
    "\n",
    "    \n",
    "data_for_training = []\n",
    "for i in os.walk(\"gmb-1.0.0/\"):\n",
    "    if not 'en.drs.xml' in i[2]:\n",
    "        continue\n",
    "    tree = ET.parse(f'{i[0]}/en.drs.xml')\n",
    "    data_for_training += prepare(tree.getroot())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal word number in sentence is 55.\n"
     ]
    }
   ],
   "source": [
    "max_text_width = max(map(lambda it: len(it), data_for_training))\n",
    "print(f'Maximal word number in sentence is {max_text_width}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "fasttext_model_name = \"model.model\"\n",
    "\n",
    "fasttext_model = FastText.load('model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'I-ART',\n",
       " 'I-DAT',\n",
       " 'I-LOC',\n",
       " 'I-MON',\n",
       " 'I-ORG',\n",
       " 'I-PCT',\n",
       " 'I-PER',\n",
       " 'I-TIM',\n",
       " 'I-TTL']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_of_labels = set()\n",
    "for cur_sample in data_for_training:\n",
    "    set_of_labels |= set(map(lambda it: it[1], cur_sample))\n",
    "set_of_labels -= {'O'}\n",
    "set_of_labels = ['O'] +  sorted(list(set_of_labels))\n",
    "set_of_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dictionary of unique words is created...'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a631f3aa0a48e5abd0f51b88512689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2965 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'There are 8565 unique words in the training data!'\n",
      "'Vectors of unique words are calculated...'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7150c2e654fd410eab7fc92746d50420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8566 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dictionary of unique words is created...'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9634c76bc1a1456cab3f23e0676ecff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1271 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'There are 5437 unique words in the training data!'\n",
      "'Vectors of unique words are calculated...'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a0a118e47e34b58bea1801928a297ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5438 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class TrainsetGenerator(Dataset):\n",
    "    def __init__(self, training_set,\n",
    "                 classes_list,\n",
    "                 used_fasttext_model: FastText, batch_size: int):\n",
    "        self.training_set = sorted(training_set, key=lambda it: len(it))\n",
    "        self.classes_list = classes_list\n",
    "        self.batch_size = batch_size\n",
    "        self.vocabulary_ = {'<BOS>': 0, '<EOS>': 1}\n",
    "        n_words = 2\n",
    "        classes_set = set(self.classes_list)\n",
    "        print(f'Dictionary of unique words is created...')\n",
    "        for cur_sent in tqdm(self.training_set):\n",
    "            for cur_word, cur_label in cur_sent:\n",
    "                if cur_word not in self.vocabulary_:\n",
    "                    self.vocabulary_[cur_word] = n_words\n",
    "                    n_words += 1\n",
    "                if cur_label not in classes_set:\n",
    "                    err_msg_ = f'Label \"{cur_label}\" is unknown!'\n",
    "                    raise ValueError(err_msg_)\n",
    "        del classes_set\n",
    "        print(f'There are {n_words - 2} unique words in the training data!')\n",
    "        all_words = sorted(list(self.vocabulary_.keys()))\n",
    "        word_vector = used_fasttext_model.wv[all_words[0]].astype(np.float64)\n",
    "        word_vector /= np.linalg.norm(word_vector)\n",
    "        word_vector = word_vector.astype(np.float32)\n",
    "        word_idx = self.vocabulary_[all_words[0]]\n",
    "        self.vector_size_ = word_vector.shape[0] + 2\n",
    "        self.matrix_ = np.zeros((n_words, self.vector_size_), dtype=np.float32)\n",
    "        self.matrix_[0, self.vector_size_ - 2] = 1.0\n",
    "        self.matrix_[1, self.vector_size_ - 1] = 1.0\n",
    "        self.matrix_[word_idx, 0:(self.vector_size_ - 2)] = word_vector\n",
    "        del word_vector\n",
    "        print(f'Vectors of unique words are calculated...')\n",
    "        for cur_word in tqdm(all_words[1:]):\n",
    "            word_vector = used_fasttext_model.wv[cur_word].astype(np.float64)\n",
    "            word_vector /= np.linalg.norm(word_vector)\n",
    "            word_vector = word_vector.astype(np.float32)\n",
    "            word_idx = self.vocabulary_[cur_word]\n",
    "            self.matrix_[word_idx, 0:(self.vector_size_ - 2)] = word_vector\n",
    "            del word_vector\n",
    "        del all_words\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.training_set) // self.batch_size) -10\n",
    "\n",
    "    def __getitem__(self, batch_idx):\n",
    "        batch_start = batch_idx * self.batch_size\n",
    "        batch_end = min(len(self.training_set), batch_start + self.batch_size)\n",
    "        max_text_len = max(map(\n",
    "            lambda it: len(it),\n",
    "            self.training_set[batch_start:batch_end]\n",
    "        ))\n",
    "        max_text_len += 2\n",
    "        batch_x = np.zeros(\n",
    "            (batch_end - batch_start, max_text_len, self.vector_size_),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        batch_y = np.zeros(\n",
    "            (batch_end - batch_start, max_text_len),\n",
    "            dtype=np.int64\n",
    "        )\n",
    "        for sent_idx, cur_sent in enumerate(self.training_set[batch_start:batch_end]):\n",
    "            time_idx = 0\n",
    "            word_idx = self.vocabulary_['<BOS>']\n",
    "            batch_x[sent_idx, time_idx] = self.matrix_[word_idx]\n",
    "            class_idx = self.classes_list.index('O')\n",
    "            batch_y[sent_idx, time_idx] = class_idx\n",
    "            for time_idx, (cur_word, cur_label) in enumerate(cur_sent):\n",
    "                word_idx = self.vocabulary_[cur_word]\n",
    "                batch_x[sent_idx, time_idx + 1] = self.matrix_[word_idx]\n",
    "                class_idx = self.classes_list.index(cur_label)\n",
    "                batch_y[sent_idx, time_idx + 1] = class_idx\n",
    "            time_idx = len(cur_sent) + 1\n",
    "            word_idx = self.vocabulary_['<EOS>']\n",
    "            batch_x[sent_idx, time_idx] = self.matrix_[word_idx]\n",
    "            class_idx = self.classes_list.index('O')\n",
    "            batch_y[sent_idx, time_idx] = class_idx\n",
    "        return torch.tensor(batch_x[0]), torch.tensor(batch_y[0])\n",
    "\n",
    "random.shuffle(data_for_training)\n",
    "n_valid = int(round(0.3 * len(data_for_training)))\n",
    "n_train = len(data_for_training) - n_valid\n",
    "train_dataloader = TrainsetGenerator(data_for_training[:n_train], set_of_labels, fasttext_model, batch_size=1)\n",
    "valid_dataloader = TrainsetGenerator(data_for_training[n_train:], set_of_labels, fasttext_model, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(102, 512, bidirectional=True)\n",
    "        self.cls = nn.Linear(1024, len(set_of_labels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "        return F.log_softmax(self.cls(rnn_out), dim = 1)\n",
    "\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "lr=0.05\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "\n",
    "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Epoch: 1/100.............'\n",
      "'Loss: 0.5554383510890749'\n",
      "'Epoch: 2/100.............'\n",
      "'Loss: 0.5400072243203317'\n",
      "'Epoch: 3/100.............'\n",
      "'Loss: 0.5333417163975852'\n",
      "'Epoch: 4/100.............'\n",
      "'Loss: 0.5258689763199171'\n",
      "'Epoch: 5/100.............'\n",
      "'Loss: 0.5231945255665895'\n",
      "'Epoch: 6/100.............'\n",
      "'Loss: 0.5148628778598081'\n",
      "'Epoch: 7/100.............'\n",
      "'Loss: 0.5099543539885992'\n",
      "'Epoch: 8/100.............'\n",
      "'Loss: 0.5139175660891676'\n",
      "'Epoch: 9/100.............'\n",
      "'Loss: 0.505844530184499'\n",
      "'Epoch: 10/100.............'\n",
      "'Loss: 0.5135514655023815'\n",
      "'Epoch: 11/100.............'\n",
      "'Loss: 0.5006406383236098'\n",
      "'Epoch: 12/100.............'\n",
      "'Loss: 0.5027646959902672'\n",
      "'Epoch: 13/100.............'\n",
      "'Loss: 0.49196123771121186'\n",
      "'Epoch: 14/100.............'\n",
      "'Loss: 0.4900650358409424'\n",
      "'Epoch: 15/100.............'\n",
      "'Loss: 0.4953741187954884'\n",
      "'Epoch: 16/100.............'\n",
      "'Loss: 0.48827745053367144'\n",
      "'Epoch: 17/100.............'\n",
      "'Loss: 0.4838273095771998'\n",
      "'Epoch: 18/100.............'\n",
      "'Loss: 0.4821291541596608'\n",
      "'Epoch: 19/100.............'\n",
      "'Loss: 0.4809724888531542'\n",
      "'Epoch: 20/100.............'\n",
      "'Loss: 0.48098672160479905'\n",
      "'Epoch: 21/100.............'\n",
      "'Loss: 0.483604438865965'\n",
      "'Epoch: 22/100.............'\n",
      "'Loss: 0.48289291211040586'\n",
      "'Epoch: 23/100.............'\n",
      "'Loss: 0.47950136602056476'\n",
      "'Epoch: 24/100.............'\n",
      "'Loss: 0.4765482886480486'\n",
      "'Epoch: 25/100.............'\n",
      "'Loss: 0.47417349598025343'\n",
      "'Epoch: 26/100.............'\n",
      "'Loss: 0.4717678265421405'\n",
      "'Epoch: 27/100.............'\n",
      "'Loss: 0.4753648299038358'\n",
      "'Epoch: 28/100.............'\n",
      "'Loss: 0.47392493167764005'\n",
      "'Epoch: 29/100.............'\n",
      "'Loss: 0.47148740120703914'\n",
      "'Epoch: 30/100.............'\n",
      "'Loss: 0.472846859016765'\n",
      "'Epoch: 31/100.............'\n",
      "'Loss: 0.47294197731767385'\n",
      "'Epoch: 32/100.............'\n",
      "'Loss: 0.4685425976146235'\n",
      "'Epoch: 33/100.............'\n",
      "'Loss: 0.4693707073351075'\n",
      "'Epoch: 34/100.............'\n",
      "'Loss: 0.4625688819583055'\n",
      "'Epoch: 35/100.............'\n",
      "'Loss: 0.46228057754066326'\n",
      "'Epoch: 36/100.............'\n",
      "'Loss: 0.4656183079324406'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mikhailp/Documents/NLP/task2/solution.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mikhailp/Documents/NLP/task2/solution.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(tag_scores, tags)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mikhailp/Documents/NLP/task2/solution.ipynb#X22sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     loss_number \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/mikhailp/Documents/NLP/task2/solution.ipynb#X22sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mikhailp/Documents/NLP/task2/solution.ipynb#X22sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mikhailp/Documents/NLP/task2/solution.ipynb#X22sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mif\u001b[39;00m epoch\u001b[39m%\u001b[39m\u001b[39m1\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss_number = 0\n",
    "    for i in range(len(train_dataloader)):\n",
    "        sentence, tags = train_dataloader[i]\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = criterion(tag_scores, tags)\n",
    "        loss_number += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch%1 == 0:\n",
    "        print('Epoch: {}/{}.............'.format(epoch, n_epochs))\n",
    "        print(\"Loss: {}\".format(loss_number/len(train_dataloader)))\n",
    "    loss_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for i in range(len(valid_dataloader)):\n",
    "        sentence, tags = valid_dataloader[i]\n",
    "        predicted = np.argmax(np.exp(model(sentence)), axis=1)\n",
    "        for p, t in zip(predicted, tags):\n",
    "            true_labels.append(set_of_labels[t])\n",
    "            predicted_labels.append(set_of_labels[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('              precision    recall  f1-score   support\\n'\n",
      " '\\n'\n",
      " '       I-DAT       0.04      0.00      0.00       880\\n'\n",
      " '       I-LOC       0.52      0.08      0.14      1193\\n'\n",
      " '       I-MON       0.70      0.68      0.69       100\\n'\n",
      " '       I-ORG       0.43      0.07      0.13       910\\n'\n",
      " '       I-PCT       0.50      0.11      0.18        37\\n'\n",
      " '       I-PER       0.65      0.14      0.24       571\\n'\n",
      " '       I-TIM       0.00      0.00      0.00        29\\n'\n",
      " '       I-TTL       0.00      0.00      0.00         1\\n'\n",
      " '           O       0.88      0.99      0.93     23365\\n'\n",
      " '\\n'\n",
      " '    accuracy                           0.87     27086\\n'\n",
      " '   macro avg       0.41      0.23      0.26     27086\\n'\n",
      " 'weighted avg       0.81      0.87      0.82     27086\\n')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikhailp/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mikhailp/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mikhailp/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
